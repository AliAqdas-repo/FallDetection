{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FD_DataOrganization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZjDFZhkU6uABzNXJJNLdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAqdas-repo/FallDetection/blob/main/FD_DataOrganization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Organization and Structuring\n",
        "The goal of this notebook is to download the dataset and organize necessary data so that it could be used to train a Deep Learning Model for Fall Detection"
      ],
      "metadata": {
        "id": "sLaem-TFSwJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Dependencies"
      ],
      "metadata": {
        "id": "sg68eTBDS7ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import csv"
      ],
      "metadata": {
        "id": "FJ1UpxzlS4ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading Dataset from Drive\n",
        "The dataset downloaded in the first block is copied directly to Google Drive to avoid downloading again and again and can be just downloaded over Google Servers from **Drive**"
      ],
      "metadata": {
        "id": "HRoa0nBYTMZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Dataset from Google Drive\n",
        "!gdown --id 1-J5TBYvi---DW68UvVrUMhyF2u-QZLdj"
      ],
      "metadata": {
        "id": "peQRR0SoTKHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzipping Dataset\n",
        "if not os.path.exists('/content/Dataset/'):\n",
        "  os.mkdir('/content/Dataset/')\n",
        "!unzip /content/dataset.zip -d /content/Dataset/"
      ],
      "metadata": {
        "id": "ZUWZI2K3Tyzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset in Colab"
      ],
      "metadata": {
        "id": "ps622PFUUNno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that Enlists Full Path of Data Files\n",
        "def list_full_path(dir):\n",
        "  return [os.path.join(dir,os.path.splitext(fi)[0]) for fi in os.listdir(dir)]"
      ],
      "metadata": {
        "id": "OIVASTS4T2B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files=list_full_path('/content/Dataset/FallAllD')"
      ],
      "metadata": {
        "id": "F9ifSLVTUD7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files[0]"
      ],
      "metadata": {
        "id": "DaxuoQpHUQxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Length of Data\n",
        "len(data_files)"
      ],
      "metadata": {
        "id": "td7u7gJKX004"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracts the Activity Code\n",
        "int(data_files[0][34:37])"
      ],
      "metadata": {
        "id": "8ZKnVKXVX59R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting Data\n",
        "Splitting entire dataset into Fall and Activities of Daily Life(ADLs)"
      ],
      "metadata": {
        "id": "5Uaxqr1NYg6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fall_files=[]\n",
        "not_fall_files=[]\n",
        "for datfile in data_files:\n",
        "    if not (datfile[-1]=='B' or datfile[-1]=='M'): #Rejecting Barometer and Magnetometer Data\n",
        "    #Splitting into Fall and Not Fall based on ActivtyID2Str.m file\n",
        "      if 100<int(datfile[34:37])<136:\n",
        "        fall_files.append(str(datfile))\n",
        "      else:\n",
        "        not_fall_files.append(str(datfile))"
      ],
      "metadata": {
        "id": "zkhdkoNAYAl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Length of All Files\n",
        "len(not_fall_files)\n",
        "#Number of Sets of Sensor Measurements. We divide by 4 because we have data from 4 sensors per subject/measurement.\n",
        "len(not_fall_files)/4"
      ],
      "metadata": {
        "id": "mUQSfCUPYd5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusing Sensor Data\n",
        "We are fusing sensor data from different sensors with different sampling rates to create a single file that can be used as input to a Neural Network"
      ],
      "metadata": {
        "id": "VDC5pCjqZa1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sorting the Data\n",
        "fall_files=sorted(fall_files)\n",
        "not_fall_files=sorted(not_fall_files)"
      ],
      "metadata": {
        "id": "LIrhjIUXZEFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_into_array(filedir):\n",
        "  #reads data into array. If file extension isn't displayed in filedir then its default to .dat\n",
        "  ext=os.path.splitext(filedir)[1]\n",
        "  if ext=='':\n",
        "    ext='.dat'\n",
        "  with open(os.path.splitext(filedir)[0]+ext,'r') as datfile :\n",
        "    file_data=csv.reader(datfile,delimiter=',')\n",
        "    data=list()\n",
        "    for row in file_data:\n",
        "      data.append([float(val) for val in row])\n",
        "  return data"
      ],
      "metadata": {
        "id": "r4QhV9KHZNRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Order of Files: Acc Bar Gyro Magn\n",
        "import cv2\n",
        "def save_combined_data(input_files,output_dir=None,AG_ONLY=True):\n",
        "############### INPUT PARAMETERS ##################\n",
        "#--------------------------------------------------\n",
        "  #input_files = Input File Paths\n",
        "  #output_dir = Directory to Output Combined Data\n",
        "  #AG_ONLY = Accelration and Gyro Data Only(True)\n",
        "#--------------------------------------------------\n",
        "\n",
        "  OUTPUT_SAMPLES=952\n",
        "  TMP_MAG_SAMPLES=4800\n",
        "  TMP_BAR_SAMPLES=1000\n",
        "\n",
        "  if AG_ONLY:\n",
        "    GAP=2\n",
        "    TOTAL_CHANNELS=6;\n",
        "\n",
        "  #Channel Assignment Order\n",
        "  #-------------------------------------\n",
        "  # Channel  1-3 - Accelerometer \n",
        "  # Channel  4-6 - Gyroscope\n",
        "  #-------------------------------------\n",
        "  else:\n",
        "    GAP=4\n",
        "    TOTAL_CHANNELS=10;\n",
        "  dataset=np.empty((len(input_files)//GAP,OUTPUT_SAMPLES,TOTAL_CHANNELS))\n",
        "  \n",
        "  #Channel Assignment Order\n",
        "  #-------------------------------------\n",
        "  # Channel  1-3 - Accelerometer \n",
        "  # Channel  4   - Barometer\n",
        "  # Channel  5-7 - Gyroscope\n",
        "  # Channel  8-10 - Magnetometer\n",
        "  #-------------------------------------\n",
        "  \n",
        "  for k in range(0,len(input_files),GAP):\n",
        "    data=np.empty((OUTPUT_SAMPLES,TOTAL_CHANNELS))\n",
        "    co_channel=0 #Current Output Channel, Used to Iterate Data in Loop\n",
        "    for ip_file in input_files[k+0:k+GAP]:\n",
        "      datpts=read_into_array(ip_file)\n",
        "      ip_datpts=[[None]*3]*OUTPUT_SAMPLES\n",
        "      if len(datpts)==200:\n",
        "        CHANNELS=1\n",
        "        tmp_bar_pts=np.asarray([[0.0]*CHANNELS]*TMP_BAR_SAMPLES)\n",
        "        #Upsample by a factor of 5 and dropping last 48 samples\n",
        "        for i in range(0,5):\n",
        "          tmp_bar_pts[i::5,0]=np.asarray(datpts)[:,0]      \n",
        "        ip_datpts=tmp_bar_pts[0:OUTPUT_SAMPLES] \n",
        "\n",
        "      elif len(datpts)==1600:\n",
        "        CHANNELS=3\n",
        "        tmp_mag_pts=[[None]*CHANNELS]*TMP_MAG_SAMPLES\n",
        "        #Zero Order Hold to Upsample Data by a Factor of 3\n",
        "        tmp_mag_pts[0::3]=datpts\n",
        "        tmp_mag_pts[1::3]=datpts\n",
        "        tmp_mag_pts[2::3]=datpts\n",
        "        #Dropping last 40 Samples and Downsampling data by factor of 5\n",
        "        ip_datpts=tmp_mag_pts[0:4760:5]\n",
        "      \n",
        "      elif len(datpts)==4760:\n",
        "        CHANNELS=3\n",
        "        ip_datpts=datpts[::5]\n",
        "      else:\n",
        "        print('Error')\n",
        "        break\n",
        "      print(ip_file)\n",
        "      data[:,co_channel:co_channel+CHANNELS]=np.asarray(ip_datpts) #assigning channels to data from files\n",
        "      co_channel=co_channel+CHANNELS\n",
        "      \n",
        "      \n",
        "    if not output_dir==None:\n",
        "      print('Saving Data')\n",
        "      if not output_dir[-1]=='/':\n",
        "        if not os.path.exists(output_dir):\n",
        "          os.makedirs(output_dir)\n",
        "        np.savetxt(f'{output_dir}/{ip_file[26:-2]}.csv',data,delimiter=\",\")\n",
        "        \n",
        "      else:\n",
        "        if not os.path.exists(output_dir):\n",
        "          os.makedirs(output_dir[:-1])\n",
        "        np.savetxt(f'{output_dir}/{ip_file[26:-2]}.csv',data,delimiter=\",\")\n",
        "        \n",
        "    else:\n",
        "      dataset[k//GAP]=data\n",
        "\n",
        "  if output_dir==None:\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Q9W_fbifaA9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_combined_data(fall_files,'/content/data_proc_AG/fall_files/',True)\n",
        "save_combined_data(not_fall_files,'/content/data_proc_AG/not_fall_files/',True)"
      ],
      "metadata": {
        "id": "LPH9gWTXZ-sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data can now be zipped and uploaded to Google Drive to save the effort for the next time you work on this project."
      ],
      "metadata": {
        "id": "axuXEPR1aUtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/data_proc_AG.zip /content/data_proc_AG/\n",
        "!cp /content/data_proc_AG.zip /content/drive/MyDrive/Datasets/FallDetect/data_proc_AG.zip"
      ],
      "metadata": {
        "id": "E6VeHyE4akd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}