{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FD_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7dC6jj8L84zpQz38S4Wma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAqdas-repo/FallDetection/blob/main/FD_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies"
      ],
      "metadata": {
        "id": "WPK58gc4pnxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "id": "zIoBGfSxpnT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Dependencies"
      ],
      "metadata": {
        "id": "Z-OM8Ve1dCVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix99qVlVc96O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import plotly.express as px\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading Processed Dataset from Google Drive\n",
        "We are downloading the dataset we prepared the Data Organization Notebook of this project. We have already prepared the dataset and made it available on Google Drive. You can simply download it over the Google Server to Colab"
      ],
      "metadata": {
        "id": "Y1yETLfAdOSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1YV106cRSbZu6txcPdi2iMA8xDOfe8-6b #Contains all Sensor Data contrary to what we did in Previous Notebook"
      ],
      "metadata": {
        "id": "Zw1nbxwqdFwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzipping the Dataset\n",
        "!unzip data_proc.zip\n",
        "!mv /content/content/data_proc /content/data_proc\n",
        "shutil.rmtree('/content/content')"
      ],
      "metadata": {
        "id": "57bRkMwBe9n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Reading Functions\n",
        "We initially define some functions to be used later for reading the processed data."
      ],
      "metadata": {
        "id": "iRkA2c15nfDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_full_path(dir):\n",
        "  return [os.path.join(dir,os.path.splitext(fi)[0]) for fi in os.listdir(dir)]"
      ],
      "metadata": {
        "id": "MfYyZORXn3OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_into_array(filedir):\n",
        "  #reads data into array. If file extension isn't displayed in filedir then its default to .dat\n",
        "  ext=os.path.splitext(filedir)[1]\n",
        "  if ext=='':\n",
        "    ext='.dat'\n",
        "  with open(os.path.splitext(filedir)[0]+ext,'r') as datfile :\n",
        "    file_data=csv.reader(datfile,delimiter=',')\n",
        "    data=list()\n",
        "    for row in file_data:\n",
        "      data.append([float(val) for val in row])\n",
        "  return data"
      ],
      "metadata": {
        "id": "oASSrbR8fJsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "We are using Plotly, a library which allows us to give amazing pictorial representation of different types of data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qE8iXLbopefZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ply_plot(datpath):\n",
        "  dat=np.asarray(read_into_array(f'{datpath}.csv'))\n",
        "  import plotly.graph_objects as go\n",
        "  if not np.shape(dat)[1]<10:\n",
        "    fig = go.Figure(data=go.Heatmap(\n",
        "                      z=dat,\n",
        "                      x=[r'$a_{x}$',r'$a_{y}$',r'$a_{z}$',r'$b$',r'$g_{x}$',r'$g_{y}$',r'$g_{z}$',r'$m_{x}$',r'$m_{y}$',r'$m_{z}$'],\n",
        "                      hoverongaps = False,colorbar={\"title\":'Sensor Reading'}))\n",
        "\n",
        "  else:\n",
        "    fig = go.Figure(data=go.Heatmap(\n",
        "                      z=dat,\n",
        "                      x=[r'$a_{x}$',r'$a_{y}$',r'$a_{z}$',r'$g_{x}$',r'$g_{y}$',r'$g_{z}$'],\n",
        "                      hoverongaps = False,colorbar={\"title\":'Sensor Reading'}))\n",
        "\n",
        "  fig.update_layout(\n",
        "      xaxis_title=r'$Sensor\\ Labels$',\n",
        "      yaxis_title=r'$Time\\ Axis$',)\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "jwlIjTx-qH4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Fall Data\n",
        "datpath=sorted(list_full_path('/content/data_proc/fall_files'))[10]\n",
        "ply_plot(datpath)"
      ],
      "metadata": {
        "id": "6QmPErj-qCQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Activities of Daily Life (ADLs) Data\n",
        "datpath=sorted(list_full_path('/content/data_proc/not_fall_files'))[0]\n",
        "ply_plot(datpath)"
      ],
      "metadata": {
        "id": "sE2H6GkbqJXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "CZtaEUdv9szk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DONOT USE PANDAS. Issue with Reading Data. Reads one point less\n",
        "def csv_dataloader(Path,auto_balance=False,skip_wrist=False):\n",
        "  ###### Parameters #######\n",
        "  # Path = Path to Data(Classes need to be split in different folders beforehand)\n",
        "  # auto_balance = Balances Data by Trimming Extra Data\n",
        "  # skip_wrist = Skips Samples from Wrist Measurements (as they are prone to errors)\n",
        "  #########################\n",
        "  classes=len(os.listdir(Path)) #number of classes\n",
        "  dirs=[os.path.join(Path,folds) for folds in os.listdir(Path)]\n",
        "  data=[]\n",
        "  lbls=[]\n",
        "  file_lbl=0\n",
        "  min_len=0\n",
        "  len_hist=[]\n",
        "  if auto_balance:\n",
        "    for dir in dirs:\n",
        "      data_files=sorted(list_full_path(dir))\n",
        "      if skip_wrist:\n",
        "        data_files=[data_file for data_file in data_files if data_file[35]!='2']\n",
        "      len_hist.append(len(data_files))\n",
        "    min_len=min(len_hist)\n",
        "  for dir in dirs:\n",
        "    data_files=sorted(list_full_path(dir))\n",
        "    if skip_wrist:\n",
        "      data_files=[data_file for data_file in data_files if data_file[35]!='2']\n",
        "    if auto_balance:\n",
        "      data_files=data_files[:min_len-1]\n",
        "    dir_data=[]\n",
        "    dir_lbls=[]\n",
        "    for data_file in data_files:\n",
        "      file_data=np.expand_dims(np.asarray(read_into_array(f'{data_file}.csv')),2)\n",
        "      dir_lbls.append(np.array(file_lbl))\n",
        "      dir_data.append(file_data)\n",
        "    file_lbl+=1\n",
        "    data.append(np.array(dir_data))\n",
        "    lbls.append(np.array(dir_lbls))\n",
        "  data=np.concatenate(tuple(data),0)\n",
        "  lbls=np.concatenate(tuple(lbls),0)  \n",
        "  return tf.data.Dataset.from_tensor_slices((data.astype(np.float32),lbls.astype(np.uint8)))"
      ],
      "metadata": {
        "id": "OuL3iZfO9wrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0=Fall 1=No Fall\n",
        "loader=csv_dataloader('/content/data_proc/',False,False)"
      ],
      "metadata": {
        "id": "PJUEwnXW-e7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Characteristics\n",
        "loader.element_spec"
      ],
      "metadata": {
        "id": "IhGi61JC-jby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting Data in Train,Validation and Test"
      ],
      "metadata": {
        "id": "fGoEmWWJ-97Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitds(dataset,train_ratio,val_ratio,test_ratio=0,shuffle=False,shuffle_buffer=5000):\n",
        "  assert train_ratio+val_ratio+test_ratio==1\n",
        "  SEED=32768;\n",
        "  if shuffle==True:\n",
        "    dataset=dataset.shuffle(shuffle_buffer,seed=SEED)\n",
        "  \n",
        "  ds_size=len(dataset)\n",
        "  train_ds=dataset.take(int(ds_size*train_ratio))\n",
        "  val_ds=dataset.skip(int(train_ratio*ds_size)).take(int(ds_size*val_ratio))\n",
        "  if not test_ratio==0:\n",
        "    test_ds=dataset.skip(int((train_ratio+val_ratio)*ds_size)).take(int(ds_size*test_ratio))\n",
        "    return train_ds,val_ds,test_ds\n",
        "\n",
        "  return train_ds,val_ds"
      ],
      "metadata": {
        "id": "GLaftHY0-q9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds,val_ds,test_ds=splitds(loader,0.8,0.1,0.1,shuffle=True,shuffle_buffer=10000)"
      ],
      "metadata": {
        "id": "1m-pFrJ4-6mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_ds))\n",
        "print(len(val_ds))\n",
        "print(len(test_ds))"
      ],
      "metadata": {
        "id": "Kskzxmqe-9Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "WnWv1c7C_NCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential(\n",
        "    [tf.keras.layers.Conv2D(16,(3,2),activation='relu',input_shape=(952,10,1)),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Conv2D(32,(3,3),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Conv2D(64,(3,1),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Conv2D(128,(3,1),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),    \n",
        "     tf.keras.layers.Conv2D(256,(3,1),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)), \n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "h53vy5JN_OGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "pN9fugJZ_Qvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(0.0005),loss='binary_crossentropy',metrics=['accuracy','FalsePositives','FalseNegatives'])"
      ],
      "metadata": {
        "id": "yRjoPlMt_dGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "PEZ6c0Y9_eWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_callback=tf.keras.callbacks.ReduceLROnPlateau('val_loss',factor=0.1,patience=2)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history=model.fit(train_ds.batch(1),validation_data=val_ds.batch(1),epochs=10,callbacks=[tensorboard_callback,lr_callback])"
      ],
      "metadata": {
        "id": "Vgucjg2q_fWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['Loss','Validation Loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['Accuracy','Validation Accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HIgntQiDQ9F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "8oQLxY-t_nKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Predictions\n",
        "We are running predictions on our trained model so we can quantify it accuracy on unseen data"
      ],
      "metadata": {
        "id": "5fJUtC48_s9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#0 = fall , 1 = No Fall\n",
        "truth_lbls=[]\n",
        "tpred_lbls=[]\n",
        "for input,lbl in test_ds.batch(1):\n",
        "  #input=tf.expand_dims(x[0],0)\n",
        "  tpred_lbls.append(model.predict(input))\n",
        "  truth_lbls.append(lbl)\n",
        "truth_lbls=np.asarray(truth_lbls)\n",
        "tpred_lbls=np.asarray(tpred_lbls)\n",
        "tpred_lbls=np.squeeze(np.squeeze(tpred_lbls,1))\n",
        "pred_lbls=np.where(tpred_lbls>=0.5,1,0)"
      ],
      "metadata": {
        "id": "iUaH-UEN_vQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lbls"
      ],
      "metadata": {
        "id": "yZ5SCzmu_0D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.asarray(tf.math.confusion_matrix(truth_lbls,pred_lbls,2)).astype(np.int16)"
      ],
      "metadata": {
        "id": "JtcNopx6__3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bininopaul https://stackoverflow.com/a/35572247/13223282\n",
        "cm=np.asarray(tf.math.confusion_matrix(truth_lbls,pred_lbls,2))\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in ['Fall','Not Fall']],\n",
        "                  columns = [i for i in ['Fall','Not Fall']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True,fmt='g')"
      ],
      "metadata": {
        "id": "tVHob8-RABSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Training using only Two Sensors Data\n",
        "Let's try using only accelerometer and gyroscope data"
      ],
      "metadata": {
        "id": "iUVJg7PsA8Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accelrometer and Gyro Only\n",
        "!gdown --id 1dICsvfTAV0ZDDNHVR6sYwix-NgyQDPwB "
      ],
      "metadata": {
        "id": "eqysRyAOBWx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data_proc_AG.zip\n",
        "!mv /content/content/data_proc_AG /content/data_proc_AG\n",
        "shutil.rmtree('/content/content')"
      ],
      "metadata": {
        "id": "wFQrzm6wBZZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot Fall Data\n",
        "datpath=sorted(list_full_path('/content/data_proc_AG/fall_files'))[10]\n",
        "ply_plot(datpath)"
      ],
      "metadata": {
        "id": "8WbDrmBpBdAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot ADLs\n",
        "datpath=sorted(list_full_path('/content/data_proc/not_fall_files'))[0]\n",
        "ply_plot(datpath)"
      ],
      "metadata": {
        "id": "ett0fIkYBkHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0=Fall 1=No Fall\n",
        "loader=csv_dataloader('/content/data_proc_AG/',False,False)"
      ],
      "metadata": {
        "id": "tvvWq5t3BsPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader.element_spec"
      ],
      "metadata": {
        "id": "zvrMT0xBBv7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds,val_ds,test_ds=splitds(loader,0.8,0.1,0.1,shuffle=True,shuffle_buffer=10000)"
      ],
      "metadata": {
        "id": "v9MoImLsByOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_ds))\n",
        "print(len(val_ds))\n",
        "print(len(test_ds))"
      ],
      "metadata": {
        "id": "y3hDAtJcB0Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential(\n",
        "    [tf.keras.layers.Conv2D(8,(3,3),strides=(1,3),activation='relu',input_shape=(952,6,1)),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Conv2D(16,(3,2),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Conv2D(32,(3,1),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Conv2D(64,(3,1),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),    \n",
        "     tf.keras.layers.Conv2D(128,(3,1),activation='relu'),\n",
        "     tf.keras.layers.MaxPool2D((2,1)),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(32,activation='relu'),\n",
        "     tf.keras.layers.Dense(16,activation='relu'),\n",
        "     tf.keras.layers.Dense(8,activation='relu'),\n",
        "     tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "eoByaN-WB1mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "DvSS5pi4B3q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(0.001),loss='binary_crossentropy',metrics=['accuracy','FalsePositives','FalseNegatives'])"
      ],
      "metadata": {
        "id": "7F7XQIJiB7GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "L8VKrIHRB9wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_callback=tf.keras.callbacks.ReduceLROnPlateau('val_loss',factor=0.1,patience=1)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history=model.fit(train_ds.batch(1),validation_data=val_ds.batch(1),epochs=10,callbacks=[tensorboard_callback,lr_callback])"
      ],
      "metadata": {
        "id": "Gz8I10uLCBEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['Loss','Validation Loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['Accuracy','Validation Accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q_ZaA928RFwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "LA7Urk-aCCsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0 = fall , 1 = No Fall\n",
        "truth_lbls=[]\n",
        "tpred_lbls=[]\n",
        "for input,lbl in test_ds.batch(1):\n",
        "  #input=tf.expand_dims(x[0],0)\n",
        "  tpred_lbls.append(model.predict(input))\n",
        "  truth_lbls.append(lbl)\n",
        "truth_lbls=np.asarray(truth_lbls)\n",
        "tpred_lbls=np.asarray(tpred_lbls)\n",
        "tpred_lbls=np.squeeze(np.squeeze(tpred_lbls,1))\n",
        "pred_lbls=np.where(tpred_lbls>=0.5,1,0)"
      ],
      "metadata": {
        "id": "KdhzzDcqCEAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Confusion Matrix\n",
        "cm=np.asarray(tf.math.confusion_matrix(truth_lbls,pred_lbls,2))\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in ['Fall','Not Fall']],\n",
        "                  columns = [i for i in ['Fall','Not Fall']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True,fmt='g')"
      ],
      "metadata": {
        "id": "CzSileNlCJS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have trained our model on both datasets. Observe which one performs better by viewing the confusion matrix. Our model is now ready to be optimized and converted to FlatBuffer format to be used in Arduino."
      ],
      "metadata": {
        "id": "toB1SA3-CMMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Converting to TensorFlow Lite "
      ],
      "metadata": {
        "id": "CRhZBdxO6OpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a Representative Dataset for Quantization"
      ],
      "metadata": {
        "id": "3yE2T8CU6gQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_ds_gen():\n",
        "  fall_files=list_full_path('/content/data_proc_AG/fall_files')\n",
        "  not_fall_files=list_full_path('/content/data_proc_AG/not_fall_files')\n",
        "  shuffled=fall_files+not_fall_files\n",
        "  for fno in range(len(shuffled)):\n",
        "    data=np.expand_dims(np.expand_dims(np.asarray(read_into_array(f'{shuffled[fno]}.csv')),2),0)\n",
        "    yield([data.astype(np.float32)])"
      ],
      "metadata": {
        "id": "ctWBIDdc6daI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Quantization and converting the model to TensorFlow Lite Format"
      ],
      "metadata": {
        "id": "p3MpmOIw6-pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter=tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_no_quant=converter.convert()\n",
        "converter.representative_dataset=representative_ds_gen\n",
        "converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
        "tflite_model=converter.convert()"
      ],
      "metadata": {
        "id": "Z6rEHOfV6lFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_date=datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "MODELS_DIR = '/content/models/'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n",
        "with open(f'/content/models/model_no_quant.tflite','wb') as f:\n",
        "  f.write(tflite_no_quant)\n",
        "with open(f'/content/models/model_quant.tflite','wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "krmTL5A66q6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_DIR ='/content/models/'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n",
        "MODEL_TFLITE = MODELS_DIR + 'model_quant.tflite'\n",
        "MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'"
      ],
      "metadata": {
        "id": "J-ugZo3h6tF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ],
      "metadata": {
        "id": "AwiHrKNG6wBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displays model data in Flatbuffer Format\n",
        "!cat {MODEL_TFLITE_MICRO}"
      ],
      "metadata": {
        "id": "gzpGvIeQ6wsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Quantized Model"
      ],
      "metadata": {
        "id": "KKyG6mkvd_A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_images\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  print('Input Details',input_details['dtype'])\n",
        "  print('Output Details',output_details['dtype'])\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = test_images[test_image_index]\n",
        "    test_label = test_labels[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    \n",
        "    test_image = np.expand_dims(test_image, axis=0)#.astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = (output > 0.6)\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "t68fYvpMeDYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to evaluate a TFLite model on all images\n",
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_images\n",
        "  global test_labels\n",
        "\n",
        "  test_image_indices = range(test_images.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "  print(predictions)\n",
        "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
        "  \n",
        "  cm=np.asarray(tf.math.confusion_matrix(test_labels,predictions,2))\n",
        "  df_cm = pd.DataFrame(cm, index = [i for i in ['Fall','Not Fall']],\n",
        "                  columns = [i for i in ['Fall','Not Fall']])\n",
        "  plt.figure(figsize = (10,7))\n",
        "  sn.heatmap(df_cm, annot=True,fmt='g')\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(test_images)))"
      ],
      "metadata": {
        "id": "HbHAi7RHeE0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images=[]\n",
        "test_labels=[]\n",
        "for test_image,test_label in test_ds:\n",
        "  test_images.append(test_image)\n",
        "  test_labels.append(test_label)\n",
        "test_images=np.asarray(test_images)\n",
        "test_labels=np.asarray(test_labels)"
      ],
      "metadata": {
        "id": "ERKJ2PfCeJsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model('/content/models/model_no_quant.tflite','Unquantized')"
      ],
      "metadata": {
        "id": "aP8W3ONu6wS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model('/content/models/model_quant.tflite','Quantized')"
      ],
      "metadata": {
        "id": "vwhjMYMSeRm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}